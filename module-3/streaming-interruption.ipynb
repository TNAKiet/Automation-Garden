{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0c9e547f",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "0c9e547f"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/langchain-ai/langchain-academy/blob/main/module-3/streaming-interruption.ipynb) [![Open in LangChain Academy](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66e9eba12c7b7688aa3dbb5e_LCA-badge-green.svg)](https://academy.langchain.com/courses/take/intro-to-langgraph/lessons/58239464-lesson-1-streaming)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319adfec-2d0a-49f2-87f9-275c4a32add2",
      "metadata": {
        "id": "319adfec-2d0a-49f2-87f9-275c4a32add2"
      },
      "source": [
        "# Streaming\n",
        "\n",
        "## Review\n",
        "\n",
        "In module 2, covered a few ways to customize graph state and memory.\n",
        "\n",
        "We built up to a Chatbot with external memory that can sustain long-running conversations.\n",
        "\n",
        "## Goals\n",
        "\n",
        "This module will dive into `human-in-the-loop`, which builds on memory and allows users to interact directly with graphs in various ways.\n",
        "\n",
        "To set the stage for `human-in-the-loop`, we'll first dive into streaming, which provides several ways to visualize graph output (e.g., node state or chat model tokens) over the course of execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa",
      "metadata": {
        "id": "db024d1f-feb3-45a0-a55c-e7712a1feefa"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install --quiet -U langgraph langchain_openai langgraph_sdk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d7e41b-c6ba-4e47-b645-6c110bede549",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "70d7e41b-c6ba-4e47-b645-6c110bede549"
      },
      "source": [
        "## Streaming\n",
        "\n",
        "LangGraph is built with [first class support for streaming](https://docs.langchain.com/oss/python/langgraph/streaming).\n",
        "\n",
        "Let's set up our Chatbot from Module 2, and show various way to stream outputs from the graph during execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5b430d92-f595-4322-a56e-06de7485daa8",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "5b430d92-f595-4322-a56e-06de7485daa8",
        "outputId": "0a26e9bc-ba6b-4d08-eefa-8ba290557c34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OPENAI_API_KEY: ··········\n"
          ]
        }
      ],
      "source": [
        "import os, getpass\n",
        "\n",
        "def _set_env(var: str):\n",
        "    if not os.environ.get(var):\n",
        "        os.environ[var] = getpass.getpass(f\"{var}: \")\n",
        "\n",
        "_set_env(\"OPENAI_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d0682fc",
      "metadata": {
        "id": "4d0682fc"
      },
      "source": [
        "Note that we use `RunnableConfig` with `call_model` to enable token-wise streaming. This is [only needed with python < 3.11](https://langchain-ai.github.io/langgraph/how-tos/streaming-tokens/). We include in case you are running this notebook in CoLab, which will use python 3.x."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
      "metadata": {
        "id": "2d7321e0-0d99-4efe-a67b-74c12271859b",
        "outputId": "fc75928d-ac77-4004-fce9-b31bf8294af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from typing import Literal\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import SystemMessage, HumanMessage, RemoveMessage\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.graph import MessagesState\n",
        "\n",
        "# LLM\n",
        "model = ChatOpenAI(model=\"gpt-5-nano\", temperature=0)\n",
        "\n",
        "# State\n",
        "class State(MessagesState):\n",
        "    summary: str\n",
        "\n",
        "# Define the logic to call the model\n",
        "def call_model(state: State, config: RunnableConfig):\n",
        "\n",
        "    # Get summary if it exists\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # If there is summary, then we add it\n",
        "    if summary:\n",
        "\n",
        "        # Add summary to system message\n",
        "        system_message = f\"Summary of conversation earlier: {summary}\"\n",
        "\n",
        "        # Append summary to any newer messages\n",
        "        messages = [SystemMessage(content=system_message)] + state[\"messages\"]\n",
        "\n",
        "    else:\n",
        "        messages = state[\"messages\"]\n",
        "\n",
        "    response = model.invoke(messages, config)\n",
        "    return {\"messages\": response}\n",
        "\n",
        "def summarize_conversation(state: State):\n",
        "\n",
        "    # First, we get any existing summary\n",
        "    summary = state.get(\"summary\", \"\")\n",
        "\n",
        "    # Create our summarization prompt\n",
        "    if summary:\n",
        "\n",
        "        # A summary already exists\n",
        "        summary_message = (\n",
        "            f\"This is summary of the conversation to date: {summary}\\n\\n\"\n",
        "            \"Extend the summary by taking into account the new messages above:\"\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        summary_message = \"Create a summary of the conversation above:\"\n",
        "\n",
        "    # Add prompt to our history\n",
        "    messages = state[\"messages\"] + [HumanMessage(content=summary_message)]\n",
        "    response = model.invoke(messages)\n",
        "\n",
        "    # Delete all but the 2 most recent messages\n",
        "    delete_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]]\n",
        "    return {\"summary\": response.content, \"messages\": delete_messages}\n",
        "\n",
        "# Determine whether to end or summarize the conversation\n",
        "def should_continue(state: State)-> Literal [\"summarize_conversation\",END]:\n",
        "\n",
        "    \"\"\"Return the next node to execute.\"\"\"\n",
        "\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # If there are more than six messages, then we summarize the conversation\n",
        "    if len(messages) > 6:\n",
        "        return \"summarize_conversation\"\n",
        "\n",
        "    # Otherwise we can just end\n",
        "    return END\n",
        "\n",
        "# Define a new graph\n",
        "workflow = StateGraph(State)\n",
        "workflow.add_node(\"conversation\", call_model)\n",
        "workflow.add_node(summarize_conversation)\n",
        "\n",
        "# Set the entrypoint as conversation\n",
        "workflow.add_edge(START, \"conversation\")\n",
        "workflow.add_conditional_edges(\"conversation\", should_continue)\n",
        "workflow.add_edge(\"summarize_conversation\", END)\n",
        "\n",
        "# Compile\n",
        "memory = MemorySaver()\n",
        "graph = workflow.compile(checkpointer=memory)\n",
        "display(Image(graph.get_graph().draw_mermaid_png()))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f847a787-b301-488c-9b58-cba9f389f55d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "f847a787-b301-488c-9b58-cba9f389f55d"
      },
      "source": [
        "### Streaming full state\n",
        "\n",
        "Now, let's talk about ways to [stream our graph state](https://docs.langchain.com/oss/python/langgraph/streaming#supported-stream-modes).\n",
        "\n",
        "`.stream` and `.astream` are sync and async methods for streaming back results.\n",
        "\n",
        "LangGraph supports a few [different streaming modes](https://docs.langchain.com/oss/python/langgraph/streaming#stream-graph-state) for graph state.\n",
        "\n",
        "* `values`: This streams the full state of the graph after each node is called.\n",
        "* `updates`: This streams updates to the state of the graph after each node is called.\n",
        "\n",
        "![values_vs_updates.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/66dbaf892d24625a201744e5_streaming1.png)\n",
        "\n",
        "Let's look at `stream_mode=\"updates\"`.\n",
        "\n",
        "Because we stream with `updates`, we only see updates to the state after node in the graph is run.\n",
        "\n",
        "Each `chunk` is a dict with `node_name` as the key and the updated state as the value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "9a6f8ae9-f244-40c5-a2da-618b72631b22",
        "outputId": "a6ddc1f5-5f3d-4c3f-c8c1-e27af778ff21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "Error code: 404 - {'error': {'message': 'The model `gpt-4o_nano` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-591418532.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Start conversation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hi! I'm Lance\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"updates\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/main.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[1;32m   2644\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch_cached_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m                         \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_writes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcached\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2646\u001b[0;31m                     for _ in runner.tick(\n\u001b[0m\u001b[1;32m   2647\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2648\u001b[0m                         \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_runner.py\u001b[0m in \u001b[0;36mtick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m                 run_with_retry(\n\u001b[0m\u001b[1;32m    168\u001b[0m                     \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m                     \u001b[0mretry_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/pregel/_retry.py\u001b[0m in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrites\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# run the task\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mParentCommand\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONF\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCONFIG_KEY_CHECKPOINT_NS\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    654\u001b[0m                     \u001b[0;31m# run in context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m                         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langgraph/_internal/_runnable.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    398\u001b[0m                 \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecurse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRunnable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1620178692.py\u001b[0m in \u001b[0;36mcall_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mmessages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m             cast(\n\u001b[1;32m    401\u001b[0m                 \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m                 self.generate_prompt(\n\u001b[0m\u001b[1;32m    403\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m   1119\u001b[0m     ) -> LLMResult:\n\u001b[1;32m   1120\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m                 results.append(\n\u001b[0;32m--> 931\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    932\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1231\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1234\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1384\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mraw_response\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"http_response\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m                 \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_response\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         if (\n\u001b[1;32m   1388\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_response_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1379\u001b[0m                 )\n\u001b[1;32m   1380\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1381\u001b[0;31m                 \u001b[0mraw_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_raw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1382\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_legacy_response.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"extra_headers\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextra_headers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLegacyAPIResponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m         )\n\u001b[0;32m-> 1294\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: Error code: 404 - {'error': {'message': 'The model `gpt-4o_nano` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
          ]
        }
      ],
      "source": [
        "# Create a thread\n",
        "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
        "\n",
        "# Start conversation\n",
        "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c4882e9-07dd-4d70-866b-dfc530418cad",
      "metadata": {
        "id": "0c4882e9-07dd-4d70-866b-dfc530418cad"
      },
      "source": [
        "Let's now just print the state update."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c859c777-cb12-4682-9108-6b367e597b81",
      "metadata": {
        "id": "c859c777-cb12-4682-9108-6b367e597b81",
        "outputId": "38198a78-6f63-46e8-bacc-fdf18ea7fe34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hi Lance! How's it going? What can I do for you today?\n"
          ]
        }
      ],
      "source": [
        "# Start conversation\n",
        "for chunk in graph.stream({\"messages\": [HumanMessage(content=\"hi! I'm Lance\")]}, config, stream_mode=\"updates\"):\n",
        "    chunk['conversation'][\"messages\"].pretty_print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583bf219-6358-4d06-ae99-c40f43569fda",
      "metadata": {
        "id": "583bf219-6358-4d06-ae99-c40f43569fda"
      },
      "source": [
        "Now, we can see `stream_mode=\"values\"`.\n",
        "\n",
        "This is the `full state` of the graph after the `conversation` node is called."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
      "metadata": {
        "id": "6ee763f8-6d1f-491e-8050-fb1439e116df",
        "outputId": "d79cc522-dafe-43f5-b62e-2f99eacf68bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm Lance\n",
            "---------------------------------------------------------------------------\n",
            "================================\u001b[1m Human Message \u001b[0m=================================\n",
            "\n",
            "hi! I'm Lance\n",
            "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
            "\n",
            "Hello, Lance! How can I assist you today?\n",
            "---------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Start conversation, again\n",
        "config = {\"configurable\": {\"thread_id\": \"2\"}}\n",
        "\n",
        "# Start conversation\n",
        "input_message = HumanMessage(content=\"hi! I'm Lance\")\n",
        "for event in graph.stream({\"messages\": [input_message]}, config, stream_mode=\"values\"):\n",
        "    for m in event['messages']:\n",
        "        m.pretty_print()\n",
        "    print(\"---\"*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "563c198a-d1a4-4700-b7a7-ff5b8e0b25d7"
      },
      "source": [
        "### Streaming tokens\n",
        "\n",
        "We often want to stream more than graph state.\n",
        "\n",
        "In particular, with chat model calls it is common to stream the tokens as they are generated.\n",
        "\n",
        "We can do this [using the `.astream_events` method](https://docs.langchain.com/oss/python/langchain/models#advanced-streaming-topics:streaming-events), which streams back events as they happen inside nodes!\n",
        "\n",
        "Each event is a dict with a few keys:\n",
        "\n",
        "* `event`: This is the type of event that is being emitted.\n",
        "* `name`: This is the name of event.\n",
        "* `data`: This is the data associated with the event.\n",
        "* `metadata`: Contains`langgraph_node`, the node emitting the event.\n",
        "\n",
        "Let's have a look."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "6ae8c7a6-c6e7-4cef-ac9f-190d2f4dd763",
        "outputId": "cd358596-b189-4ce2-c32f-0d3ebe12b514"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Node: . Type: on_chain_start. Name: LangGraph\n",
            "Node: conversation. Type: on_chain_start. Name: conversation\n",
            "Node: conversation. Type: on_chat_model_start. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_stream. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chat_model_end. Name: ChatOpenAI\n",
            "Node: conversation. Type: on_chain_start. Name: should_continue\n",
            "Node: conversation. Type: on_chain_end. Name: should_continue\n",
            "Node: conversation. Type: on_chain_stream. Name: conversation\n",
            "Node: conversation. Type: on_chain_end. Name: conversation\n",
            "Node: . Type: on_chain_stream. Name: LangGraph\n",
            "Node: . Type: on_chain_end. Name: LangGraph\n"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"3\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    print(f\"Node: {event['metadata'].get('langgraph_node','')}. Type: {event['event']}. Name: {event['name']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d",
      "metadata": {
        "id": "0b63490f-3d24-4f68-95ca-5320ccb61d2d"
      },
      "source": [
        "The central point is that tokens from chat models within your graph have the `on_chat_model_stream` type.\n",
        "\n",
        "We can use `event['metadata']['langgraph_node']` to select the node to stream from.\n",
        "\n",
        "And we can use `event['data']` to get the actual data for each event, which in this case is an `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
      "metadata": {
        "id": "cc3529f8-3960-4d41-9ed6-373f93183950",
        "outputId": "27006117-940a-4092-d5f0-734731a33874"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' professional', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' American', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' based', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Bay', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Area', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' They', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' compete', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' League', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' club', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' league', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' National', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='N', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' division', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' was', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' founded', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='6', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' as', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' charter', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' member', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' All', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='-Amer', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ica', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Football', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Conference', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='AA', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='FC', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=')', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' joined', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='194', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' when', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' leagues', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' merged', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='###', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Key', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Points', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=':\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='St', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='adium', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' play', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' home', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' games', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Levi', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Stadium', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Santa', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Clara', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' California', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' which', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' moved', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='4', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Before', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' that', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' played', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' at', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Cand', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='lestick', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Park', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' San', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Francisco', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='Team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Colors', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Masc', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ot', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' colors', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' red', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' gold', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' white', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' mascot', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' S', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ourd', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ough', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Sam', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='Champ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ionship', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' won', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' five', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' championships', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' XVI', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' XIX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='III', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='IV', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' XX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='IX', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='),', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' most', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' successful', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' period', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' being', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='198', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' early', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='199', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='0', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='s', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='Not', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='able', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Figures', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' team', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' has', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' had', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' several', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Hall', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Fame', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' players', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Joe', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Montana', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Jerry', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Rice', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Steve', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Young', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Ronnie', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' L', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ott', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Charles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Haley', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Bill', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Walsh', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' legendary', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' head', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' coach', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' credited', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' developing', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' West', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Coast', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' offense', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' which', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' became', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' staple', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=\" team's\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' success', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='R', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ival', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' notable', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' rival', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ries', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Seattle', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Seahawks', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Los', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Angeles', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Rams', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' historically', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' with', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Dallas', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Cowboys', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Green', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Bay', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Packers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='-', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' **', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='Recent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Performance', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='**', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=':', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' In', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' recent', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' years', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' have', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' experienced', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' resurgence', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' including', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' appearance', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='201', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='9', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' season', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' (', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='Super', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Bowl', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' LIV', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='),', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' where', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' they', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' were', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' defeated', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' by', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Kansas', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' City', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' Chiefs', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.\\n\\n', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='The', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' ', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='49', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='ers', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' are', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' known', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' for', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' their', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' rich', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' history', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' passionate', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' fan', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' base', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=',', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' significant', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' contributions', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' NFL', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=\"'s\", additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' development', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' and', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content=' popularity', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8')}\n",
            "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'service_tier': 'default', 'model_provider': 'openai'}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8', chunk_position='last')}\n",
            "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8', usage_metadata={'input_tokens': 16, 'output_tokens': 382, 'total_tokens': 398, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})}\n",
            "{'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='lc_run--2dc940a7-76b1-4cb1-8e3c-4c18bd4598a8', chunk_position='last')}\n"
          ]
        }
      ],
      "source": [
        "node_to_stream = 'conversation'\n",
        "config = {\"configurable\": {\"thread_id\": \"4\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Get chat model tokens from a particular node\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
        "        print(event[\"data\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "226e569a-76c3-43d8-8f89-3ae687efde1c",
      "metadata": {
        "id": "226e569a-76c3-43d8-8f89-3ae687efde1c"
      },
      "source": [
        "As you see above, just use the `chunk` key to get the `AIMessageChunk`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
      "metadata": {
        "id": "3aeae53d-6dcf-40d0-a0c6-c40de492cc83",
        "outputId": "cbe5e330-10f2-420e-88b4-86de98bb1882"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|The| San| Francisco| |49|ers| are| a| professional| American| football| team| based| in| the| San| Francisco| Bay| Area|.| They| compete| in| the| National| Football| League| (|NFL|)| as| a| member| club| of| the| league|'s| National| Football| Conference| (|N|FC|)| West| division|.| The| team| was| founded| in| |194|6| as| a| charter| member| of| the| All|-Amer|ica| Football| Conference| (|AA|FC|)| and| joined| the| NFL| in| |194|9| when| the| leagues| merged|.\n",
            "\n",
            "|The| |49|ers| have| a| rich| history| and| are| one| of| the| most| successful| teams| in| NFL| history|.| They| have| won| five| Super| Bowl| championships|,| with| victories| in| Super| Bow|ls| XVI|,| XIX|,| XX|III|,| XX|IV|,| and| XX|IX|.| The| team| has| also| won| numerous| division| titles| and| made| several| playoff| appearances|.\n",
            "\n",
            "|The| |49|ers| are| known| for| their| iconic| players| and| coaches|,| including| Hall| of| Fam|ers| like| Joe| Montana|,| Jerry| Rice|,| Steve| Young|,| Ronnie| L|ott|,| and| coach| Bill| Walsh|,| who| is| credited| with| popular|izing| the| West| Coast| offense|.| The| team's| colors| are| red| and| gold|,| and| they| play| their| home| games| at| Levi|'s| Stadium| in| Santa| Clara|,| California|,| which| they| moved| to| in| |201|4| after| previously| playing| at| Cand|lestick| Park| in| San| Francisco|.\n",
            "\n",
            "|The| |49|ers| have| a| passionate| fan| base| and| a| stor|ied| rivalry| with| teams| like| the| Dallas| Cowboys|,| Green| Bay| Packers|,| and| Seattle| Seahawks|.| The| team's| success| in| the| |198|0|s| and| |199|0|s|,| particularly| under| the| leadership| of| Bill| Walsh| and| George| Se|if|ert|,| helped| establish| them| as| one| of| the| premier| franchises| in| the| NFL|.||||"
          ]
        }
      ],
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"5\"}}\n",
        "input_message = HumanMessage(content=\"Tell me about the 49ers NFL team\")\n",
        "async for event in graph.astream_events({\"messages\": [input_message]}, config, version=\"v2\"):\n",
        "    # Get chat model tokens from a particular node\n",
        "    if event[\"event\"] == \"on_chat_model_stream\" and event['metadata'].get('langgraph_node','') == node_to_stream:\n",
        "        data = event[\"data\"]\n",
        "        print(data[\"chunk\"].content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "5826e4d8-846b-4f6c-a5c1-e781d43022db"
      },
      "source": [
        "### Streaming with LangGraph API\n",
        "\n",
        "**⚠️ Notice**\n",
        "\n",
        "Since filming these videos, we've updated Studio so that it can now be run locally and accessed through your browser. This is the preferred way to run Studio instead of using the Desktop App shown in the video. It is now called _LangSmith Studio_ instead of _LangGraph Studio_. Detailed setup instructions are available in the \"Getting Setup\" guide at the start of the course. You can find a description of Studio [here](https://docs.langchain.com/langsmith/studio), and specific details for local deployment [here](https://docs.langchain.com/langsmith/quick-start-studio#local-development-server).  \n",
        "To start the local development server, run the following command in your terminal in the `/studio` directory in this module:\n",
        "\n",
        "```\n",
        "langgraph dev\n",
        "```\n",
        "\n",
        "You should see the following output:\n",
        "```\n",
        "- 🚀 API: http://127.0.0.1:2024\n",
        "- 🎨 Studio UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024\n",
        "- 📚 API Docs: http://127.0.0.1:2024/docs\n",
        "```\n",
        "\n",
        "Open your browser and navigate to the **Studio UI** URL shown above.\n",
        "\n",
        "The LangGraph API  [supports editing graph state](https://docs.langchain.com/langsmith/add-human-in-the-loop)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8925b632-512b-48e1-9220-61c06bfbf0b8",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8925b632-512b-48e1-9220-61c06bfbf0b8"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    raise Exception(\"Unfortunately LangGraph Studio is currently not supported on Google Colab\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079c2ad6",
      "metadata": {
        "id": "079c2ad6"
      },
      "outputs": [],
      "source": [
        "from langgraph_sdk import get_client\n",
        "\n",
        "# This is the URL of the local development server\n",
        "URL = \"http://127.0.0.1:2024\"\n",
        "client = get_client(url=URL)\n",
        "\n",
        "# Search all hosted graphs\n",
        "assistants = await client.assistants.search()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4d15af9e-0e86-41e3-a5ba-ee2a4aa08a32"
      },
      "source": [
        "Let's [stream `values`](https://docs.langchain.com/oss/python/langgraph/streaming#stream-graph-state), like before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "63e3096f-5429-4d3c-8de2-2bddf7266ebf",
        "outputId": "c1df153c-d4e0-40a6-bc1f-437b075637ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StreamPart(event='metadata', data={'run_id': '019a0358-31b4-7143-af47-2feeac0b27ce', 'attempt': 1})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '9aaa247f-1e6e-4451-af25-ac678fe46d82'}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '9aaa247f-1e6e-4451-af25-ac678fe46d82'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CSqw4HhOXHahA4tIr4mIdEhQB1QB4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--4a3794f6-52c3-41b5-9620-1710d6e8392d-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_3sCWZZ89AoUe91MYc3ZBtJ0P', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '9aaa247f-1e6e-4451-af25-ac678fe46d82'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CSqw4HhOXHahA4tIr4mIdEhQB1QB4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--4a3794f6-52c3-41b5-9620-1710d6e8392d-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_3sCWZZ89AoUe91MYc3ZBtJ0P', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'f05da6b3-27be-4896-967a-3ff60aa06d85', 'tool_call_id': 'call_3sCWZZ89AoUe91MYc3ZBtJ0P', 'artifact': None, 'status': 'success'}]})\n",
            "StreamPart(event='values', data={'messages': [{'content': 'Multiply 2 and 3', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'human', 'name': None, 'id': '9aaa247f-1e6e-4451-af25-ac678fe46d82'}, {'content': '', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CSqw4HhOXHahA4tIr4mIdEhQB1QB4', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--4a3794f6-52c3-41b5-9620-1710d6e8392d-0', 'tool_calls': [{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_3sCWZZ89AoUe91MYc3ZBtJ0P', 'type': 'tool_call'}], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}, {'content': '6', 'additional_kwargs': {}, 'response_metadata': {}, 'type': 'tool', 'name': 'multiply', 'id': 'f05da6b3-27be-4896-967a-3ff60aa06d85', 'tool_call_id': 'call_3sCWZZ89AoUe91MYc3ZBtJ0P', 'artifact': None, 'status': 'success'}, {'content': 'The result of multiplying 2 and 3 is 6.', 'additional_kwargs': {'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 14, 'prompt_tokens': 159, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CSqw4xk8t3sPODL5beks5TlJBozgB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, 'type': 'ai', 'name': None, 'id': 'lc_run--33a25ab0-f748-4c7f-a086-d9249e25fdc0-0', 'tool_calls': [], 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 159, 'output_tokens': 14, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}}]})\n"
          ]
        }
      ],
      "source": [
        "# Create a new thread\n",
        "thread = await client.threads.create()\n",
        "# Input message\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      assistant_id=\"agent\",\n",
        "                                      input={\"messages\": [input_message]},\n",
        "                                      stream_mode=\"values\"):\n",
        "    print(event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556dc7fd-1cae-404f-816a-f13d772b3b14",
      "metadata": {
        "id": "556dc7fd-1cae-404f-816a-f13d772b3b14"
      },
      "source": [
        "The streamed objects have:\n",
        "\n",
        "* `event`: Type\n",
        "* `data`: State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b735aa-139c-45a3-a850-63519c0004f0",
      "metadata": {
        "id": "57b735aa-139c-45a3-a850-63519c0004f0",
        "outputId": "9dd7b982-628b-481b-de38-650c59040cd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=========================\n",
            "content='Multiply 2 and 3' additional_kwargs={} response_metadata={} id='c3ec872a-99a1-4eec-bcb6-a04973f48ac5'\n",
            "=========================\n",
            "content='' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 134, 'output_tokens': 17, 'total_tokens': 151, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 134, 'total_tokens': 151, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_f64f290af2', 'id': 'chatcmpl-CSqw6HYoyCI7z2AuKAAfSTQGbvzla', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--c91028e7-7a0a-4746-a4f5-edcff5380abc-0' tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 3}, 'id': 'call_AFChrxIQGbr7mmzr8buxymY0', 'type': 'tool_call'}]\n",
            "=========================\n",
            "content='6' name='multiply' id='f69a844b-5f82-4256-96dd-92b044e888d9' tool_call_id='call_AFChrxIQGbr7mmzr8buxymY0'\n",
            "=========================\n",
            "content='The result of multiplying 2 and 3 is 6.' additional_kwargs={'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 159, 'output_tokens': 14, 'total_tokens': 173, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 159, 'total_tokens': 173, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_eb3c3cb84d', 'id': 'chatcmpl-CSqw7xBeinGuHlx0upkmo2tryppco', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--dba1c4af-ed8f-4fed-8770-2c3429603cd0-0'\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import convert_to_messages\n",
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"], assistant_id=\"agent\", input={\"messages\": [input_message]}, stream_mode=\"values\"):\n",
        "    messages = event.data.get('messages',None)\n",
        "    if messages:\n",
        "        print(convert_to_messages(messages)[-1])\n",
        "    print('='*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a555d186-27be-4ddf-934c-895a3105035d",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "a555d186-27be-4ddf-934c-895a3105035d"
      },
      "source": [
        "There are some new streaming mode that are only supported via the API.\n",
        "\n",
        "For example, we can  [use `messages` mode](https://docs.langchain.com/oss/python/langgraph/streaming#supported-stream-modes) to better handle the above case!\n",
        "\n",
        "This mode currently assumes that you have a `messages` key in your graph, which is a list of messages.\n",
        "\n",
        "All events emitted using `messages` mode have two attributes:\n",
        "\n",
        "* `event`: This is the name of the event\n",
        "* `data`: This is data associated with the event"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "4abd91f6-63c0-41ee-9988-7c8248b88a45",
        "outputId": "500a739e-b193-4f15-eeef-fdab934eeca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "metadata\n",
            "messages/metadata\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/metadata\n",
            "messages/complete\n",
            "messages/metadata\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n",
            "messages/partial\n"
          ]
        }
      ],
      "source": [
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "async for event in client.runs.stream(thread[\"thread_id\"],\n",
        "                                      assistant_id=\"agent\",\n",
        "                                      input={\"messages\": [input_message]},\n",
        "                                      stream_mode=\"messages\"):\n",
        "    print(event.event)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8de2f1ea-b232-43fc-af7a-320efce83381",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "8de2f1ea-b232-43fc-af7a-320efce83381"
      },
      "source": [
        "We can see a few events:\n",
        "\n",
        "* `metadata`: metadata about the run\n",
        "* `messages/complete`: fully formed message\n",
        "* `messages/partial`: chat model tokens\n",
        "\n",
        "<!--You can dig further into the types [~here~](https://langchain-ai.github.io/langgraph/cloud/concepts/api/#modemessages) [here](https://docs.langchain.com/oss/python/langgraph/concepts/langgraph_server). -->\n",
        "\n",
        "Now, let's show how to stream these messages.\n",
        "\n",
        "We'll define a helper function for better formatting of the tool calls in messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
      "metadata": {
        "editable": true,
        "tags": [],
        "id": "50a85e16-6e3f-4f14-bcf9-8889a762f522",
        "outputId": "8c597643-afa1-4b04-cd9f-2d304306d737"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metadata: Run ID - 019a0358-57dc-76f9-bc63-633eee467a86\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
            "Response Metadata: Finish Reason - tool_calls\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
            "Response Metadata: Finish Reason - tool_calls\n",
            "--------------------------------------------------\n",
            "Tool Calls:\n",
            "Tool Call ID: call_mDtKBiuzkpN5ITWykhEFN0XU, Function: multiply, Arguments: {'a': 2, 'b': 3}\n",
            "Response Metadata: Finish Reason - tool_calls\n",
            "--------------------------------------------------\n",
            "--------------------------------------------------\n",
            "AI: The\n",
            "--------------------------------------------------\n",
            "AI: The result\n",
            "--------------------------------------------------\n",
            "AI: The result of\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying \n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and \n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is \n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is 6\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is 6.\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is 6.\n",
            "Response Metadata: Finish Reason - stop\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is 6.\n",
            "Response Metadata: Finish Reason - stop\n",
            "--------------------------------------------------\n",
            "AI: The result of multiplying 2 and 3 is 6.\n",
            "Response Metadata: Finish Reason - stop\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "thread = await client.threads.create()\n",
        "input_message = HumanMessage(content=\"Multiply 2 and 3\")\n",
        "\n",
        "def format_tool_calls(tool_calls):\n",
        "    \"\"\"\n",
        "    Format a list of tool calls into a readable string.\n",
        "\n",
        "    Args:\n",
        "        tool_calls (list): A list of dictionaries, each representing a tool call.\n",
        "            Each dictionary should have 'id', 'name', and 'args' keys.\n",
        "\n",
        "    Returns:\n",
        "        str: A formatted string of tool calls, or \"No tool calls\" if the list is empty.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if tool_calls:\n",
        "        formatted_calls = []\n",
        "        for call in tool_calls:\n",
        "            formatted_calls.append(\n",
        "                f\"Tool Call ID: {call['id']}, Function: {call['name']}, Arguments: {call['args']}\"\n",
        "            )\n",
        "        return \"\\n\".join(formatted_calls)\n",
        "    return \"No tool calls\"\n",
        "\n",
        "async for event in client.runs.stream(\n",
        "    thread[\"thread_id\"],\n",
        "    assistant_id=\"agent\",\n",
        "    input={\"messages\": [input_message]},\n",
        "    stream_mode=\"messages\",):\n",
        "\n",
        "    # Handle metadata events\n",
        "    if event.event == \"metadata\":\n",
        "        print(f\"Metadata: Run ID - {event.data['run_id']}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Handle partial message events\n",
        "    elif event.event == \"messages/partial\":\n",
        "        for data_item in event.data:\n",
        "            # Process user messages\n",
        "            if \"role\" in data_item and data_item[\"role\"] == \"user\":\n",
        "                print(f\"Human: {data_item['content']}\")\n",
        "            else:\n",
        "                # Extract relevant data from the event\n",
        "                tool_calls = data_item.get(\"tool_calls\", [])\n",
        "                invalid_tool_calls = data_item.get(\"invalid_tool_calls\", [])\n",
        "                content = data_item.get(\"content\", \"\")\n",
        "                response_metadata = data_item.get(\"response_metadata\", {})\n",
        "\n",
        "                if content:\n",
        "                    print(f\"AI: {content}\")\n",
        "\n",
        "                if tool_calls:\n",
        "                    print(\"Tool Calls:\")\n",
        "                    print(format_tool_calls(tool_calls))\n",
        "\n",
        "                if invalid_tool_calls:\n",
        "                    print(\"Invalid Tool Calls:\")\n",
        "                    print(format_tool_calls(invalid_tool_calls))\n",
        "\n",
        "                if response_metadata and response_metadata.get(\"finish_reason\"):\n",
        "                    print(f\"Response Metadata: Finish Reason - {response_metadata['finish_reason']}\")\n",
        "        print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18",
      "metadata": {
        "id": "1ae885f8-102f-448a-9d68-8ded8d2bbd18"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}